{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RL Crash Course Q Learning and DQN Policy Gradient Actor-Critic PPO GRPO","title":"RL Crash Course"},{"location":"#rl-crash-course","text":"Q Learning and DQN Policy Gradient Actor-Critic PPO GRPO","title":"RL Crash Course"},{"location":"01/","text":"RL Ch1 QLearning RL and Its Components RL is a online-learning system based on trial and error. RL process can be analogized as a game, where the player (agent) interacts with the environment to achieve a goal. The agent can take actions and receive rewards from the environment. The agent's goal is to maximize the total reward it receives over time. RL can be used on both traditional ML algorithms and NN based methods. This series of notes only focus on the latter. We usually call each round of a game an episode. Gymnasium A popular library for RL is OpenAI's Gymnasium. It provides a wide range of environments for testing RL algorithms. The environments are categorized into different levels of difficulty. You can simply install gymnasium[box2d] swig to get started, we would also need pytorch . To create a game (an environment) in Gymnasium, you can use the following code: import gym env = gym.make('CartPole-v0') There is an option, render_mode , that can be set to human to visualize the game. We usually only use the mode human to visualize the game, and let it stay unrendered ( None ) in other cases. Useful methods for the environment object include: observation, info = env.reset() reset the environment and return the initial observation (a state tensor). observation, reward, terminated, truncated, info = env.step(action) take an action and return the observation, reward (a scaler), whether the games failed (terminated), whether the game is truncated (run out of time), and some additional information. env.action_space.shape check the accepted shape of the action tensor. action = env.action_space.sample() generate a random action. env.observation_space has the same methods mentioned above for action space. Q-Learning The first RL algorithm we will discuss is Q-Learning. Q-Learning is a model-free, off-policy algorithm that can be used to find the optimal action-selection policy for any given MDP. The Q-Learning algorithm is based on the Bellman equation, which is used to calculate the Q-value of a state-action pair. Q-Learning was originally proposed by Chris Watkins in 1989, applied to traditional ML algorithms. However, we use NN instead here. Q-Learning aims to give every action a Q-value, which represents the expected total reward the agent will receive if it takes that action in a given state. The agent then selects the action with the highest Q-value, known as the greedy exploitation. However, during training, this typically prohibits the model to choose other options, so the stratagem used during training is usually randomized-greedy. That is, having a small chance of randomly choosing an action, or most of the time, choose the action greedily. The Q-Learning algorithm is based on the following equation: $$Q_{next}(s, a) = Q(s, a) + \\alpha \\cdot (r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a))$$ where: $Q(s, a)$ is the Q-value of the state-action pair $(s, a)$. $\\alpha$ is the learning rate. $r$ is the reward received after taking action $a$ in state $s$. $\\gamma$ is the discount factor. $s'$ is the next state. $a'$ is the next action. This equation means that, the expected Q-value for the current $a, s$ pair is expected to be $r + \\gamma \\cdot \\max_{a'} Q(s', a')$, which means is the current reward plus the most optimal Q-value of the future with a decay of $gamma$. This is for the more traditional ML method. For the NN method, we use the following loss function: $$L = \\left( r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right)^2$$ Obviously this is just a rewritten form for the previous equation, when we assume the output of the model is a Q-value tensor, because if we take gradient for that loss, the update step becomes the same as the previous equation. $a'$ can be approximated by the current model. That is, what the current model would do under the given state $s'$. Implementation DQN means Deep Q-learning Network. It is a NN based Q-learning algorithm. We will implement it here. This is just a simple MLP model. You can use any other architecture you like. The training process is identical to previous equations, except that the max function is replaced by an approximation of the current model (as the reference model). The loss curve may bounce up and down for RL, but you can witness a steady increase in the reward, which is more important. For this scenario, the Q-Learning method yields an acceptable result.s import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import random from torch.optim.lr_scheduler import CosineAnnealingLR import wandb from tqdm import trange import os wandb.init(project=\"rl\") device = \"cpu\" print(f\"Using device: {device}\") class QNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(QNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.fc3 = nn.Linear(128, action_dim) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x num_episodes = 1000 learning_rate = 3e-4 gamma = 0.99 epsilon = 1.0 epsilon_decay = 0.99 min_epsilon = 0.05 env = gym.make(\"CartPole-v1\") state_dim = env.observation_space.shape[0] action_dim = env.action_space.n q_network = QNetwork(state_dim, action_dim).to(device) if os.path.exists(\"q_network.pth\"): q_network.load_state_dict(torch.load(\"q_network.pth\")) optimizer = optim.Adam(q_network.parameters(), lr=learning_rate) scheduler = CosineAnnealingLR(optimizer, num_episodes) loss_fn = nn.MSELoss() for episode in trange(num_episodes): state, _ = env.reset() state = torch.FloatTensor(state).to(device) episode_reward = 0 done = False while not done: if random.random() < epsilon: action = env.action_space.sample() else: with torch.no_grad(): q_values = q_network(state) action = torch.argmax(q_values).item() next_state, reward, done, _, _ = env.step(action) next_state_tensor = torch.FloatTensor(next_state).to(device) episode_reward += reward with torch.no_grad(): q_next = q_network(next_state_tensor) max_q_next = torch.max(q_next).item() target_q = reward + (gamma * max_q_next if not done else 0.0) q_values = q_network(state) current_q = q_values[action] loss = loss_fn(current_q, torch.tensor(target_q).to(device)) wandb.log({\"loss\": loss.item(), \"episode\": episode, \"lr\": scheduler.get_last_lr()[0], \"epsilon\": epsilon, \"reward\": episode_reward}) optimizer.zero_grad() loss.backward() optimizer.step() state = next_state_tensor scheduler.step() epsilon = max(min_epsilon, epsilon * epsilon_decay) env.close() q_network.eval() torch.save(q_network.state_dict(), \"q_network.pth\") test_env = gym.make(\"CartPole-v1\", render_mode=\"human\") state, _ = test_env.reset() done = False while not done: test_env.render() state_tensor = torch.FloatTensor(state).to(device) with torch.no_grad(): q_values = q_network(state_tensor) action = torch.argmax(q_values).item() state, reward, done, _, _ = test_env.step(action) test_env.close()","title":"RL Ch1 QLearning"},{"location":"01/#rl-ch1-qlearning","text":"","title":"RL Ch1 QLearning"},{"location":"01/#rl-and-its-components","text":"RL is a online-learning system based on trial and error. RL process can be analogized as a game, where the player (agent) interacts with the environment to achieve a goal. The agent can take actions and receive rewards from the environment. The agent's goal is to maximize the total reward it receives over time. RL can be used on both traditional ML algorithms and NN based methods. This series of notes only focus on the latter. We usually call each round of a game an episode.","title":"RL and Its Components"},{"location":"01/#gymnasium","text":"A popular library for RL is OpenAI's Gymnasium. It provides a wide range of environments for testing RL algorithms. The environments are categorized into different levels of difficulty. You can simply install gymnasium[box2d] swig to get started, we would also need pytorch . To create a game (an environment) in Gymnasium, you can use the following code: import gym env = gym.make('CartPole-v0') There is an option, render_mode , that can be set to human to visualize the game. We usually only use the mode human to visualize the game, and let it stay unrendered ( None ) in other cases. Useful methods for the environment object include: observation, info = env.reset() reset the environment and return the initial observation (a state tensor). observation, reward, terminated, truncated, info = env.step(action) take an action and return the observation, reward (a scaler), whether the games failed (terminated), whether the game is truncated (run out of time), and some additional information. env.action_space.shape check the accepted shape of the action tensor. action = env.action_space.sample() generate a random action. env.observation_space has the same methods mentioned above for action space.","title":"Gymnasium"},{"location":"01/#q-learning","text":"The first RL algorithm we will discuss is Q-Learning. Q-Learning is a model-free, off-policy algorithm that can be used to find the optimal action-selection policy for any given MDP. The Q-Learning algorithm is based on the Bellman equation, which is used to calculate the Q-value of a state-action pair. Q-Learning was originally proposed by Chris Watkins in 1989, applied to traditional ML algorithms. However, we use NN instead here. Q-Learning aims to give every action a Q-value, which represents the expected total reward the agent will receive if it takes that action in a given state. The agent then selects the action with the highest Q-value, known as the greedy exploitation. However, during training, this typically prohibits the model to choose other options, so the stratagem used during training is usually randomized-greedy. That is, having a small chance of randomly choosing an action, or most of the time, choose the action greedily. The Q-Learning algorithm is based on the following equation: $$Q_{next}(s, a) = Q(s, a) + \\alpha \\cdot (r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a))$$ where: $Q(s, a)$ is the Q-value of the state-action pair $(s, a)$. $\\alpha$ is the learning rate. $r$ is the reward received after taking action $a$ in state $s$. $\\gamma$ is the discount factor. $s'$ is the next state. $a'$ is the next action. This equation means that, the expected Q-value for the current $a, s$ pair is expected to be $r + \\gamma \\cdot \\max_{a'} Q(s', a')$, which means is the current reward plus the most optimal Q-value of the future with a decay of $gamma$. This is for the more traditional ML method. For the NN method, we use the following loss function: $$L = \\left( r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a) \\right)^2$$ Obviously this is just a rewritten form for the previous equation, when we assume the output of the model is a Q-value tensor, because if we take gradient for that loss, the update step becomes the same as the previous equation. $a'$ can be approximated by the current model. That is, what the current model would do under the given state $s'$.","title":"Q-Learning"},{"location":"01/#implementation","text":"DQN means Deep Q-learning Network. It is a NN based Q-learning algorithm. We will implement it here. This is just a simple MLP model. You can use any other architecture you like. The training process is identical to previous equations, except that the max function is replaced by an approximation of the current model (as the reference model). The loss curve may bounce up and down for RL, but you can witness a steady increase in the reward, which is more important. For this scenario, the Q-Learning method yields an acceptable result.s import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import random from torch.optim.lr_scheduler import CosineAnnealingLR import wandb from tqdm import trange import os wandb.init(project=\"rl\") device = \"cpu\" print(f\"Using device: {device}\") class QNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(QNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.fc3 = nn.Linear(128, action_dim) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x num_episodes = 1000 learning_rate = 3e-4 gamma = 0.99 epsilon = 1.0 epsilon_decay = 0.99 min_epsilon = 0.05 env = gym.make(\"CartPole-v1\") state_dim = env.observation_space.shape[0] action_dim = env.action_space.n q_network = QNetwork(state_dim, action_dim).to(device) if os.path.exists(\"q_network.pth\"): q_network.load_state_dict(torch.load(\"q_network.pth\")) optimizer = optim.Adam(q_network.parameters(), lr=learning_rate) scheduler = CosineAnnealingLR(optimizer, num_episodes) loss_fn = nn.MSELoss() for episode in trange(num_episodes): state, _ = env.reset() state = torch.FloatTensor(state).to(device) episode_reward = 0 done = False while not done: if random.random() < epsilon: action = env.action_space.sample() else: with torch.no_grad(): q_values = q_network(state) action = torch.argmax(q_values).item() next_state, reward, done, _, _ = env.step(action) next_state_tensor = torch.FloatTensor(next_state).to(device) episode_reward += reward with torch.no_grad(): q_next = q_network(next_state_tensor) max_q_next = torch.max(q_next).item() target_q = reward + (gamma * max_q_next if not done else 0.0) q_values = q_network(state) current_q = q_values[action] loss = loss_fn(current_q, torch.tensor(target_q).to(device)) wandb.log({\"loss\": loss.item(), \"episode\": episode, \"lr\": scheduler.get_last_lr()[0], \"epsilon\": epsilon, \"reward\": episode_reward}) optimizer.zero_grad() loss.backward() optimizer.step() state = next_state_tensor scheduler.step() epsilon = max(min_epsilon, epsilon * epsilon_decay) env.close() q_network.eval() torch.save(q_network.state_dict(), \"q_network.pth\") test_env = gym.make(\"CartPole-v1\", render_mode=\"human\") state, _ = test_env.reset() done = False while not done: test_env.render() state_tensor = torch.FloatTensor(state).to(device) with torch.no_grad(): q_values = q_network(state_tensor) action = torch.argmax(q_values).item() state, reward, done, _, _ = test_env.step(action) test_env.close()","title":"Implementation"},{"location":"02/","text":"RL Ch2 Policy Gradient Policy Gradient Q-learning is simple but has its limitations. The major one being that, it only allows for discrete actions. For environments like MountainCarContinuous-v0 , it takes continuous acceleration value from negative one to positive one, and thus disallowing Q-learning. Policy gradient algorithm is still model-free, online-learning, and trial-and-error based, but it allows for continuous actions. Our model should give our a distribution of all the actions. That is, for continuous case, giving out the mean and variance for each input (assume we ust gaussian distribution), and for discreet case, giving out a softmax-ed vector. The major equation is the loss function: $$ L(\\theta) = -\\mathbb{E} \\big[ \\log \\pi_\\theta(a|s) \\cdot A \\big] $$ Here: - $\\pi_\\theta(a|s)$: The policy, which gives the probability of taking action $a$ in state $s$. - $A$: The advantage value. For now, it can equal to the cumulative reward for the given action. - The negative sign ensures we maximize rewards (since optimization minimizes the loss). - $\\mathbb{E}$ is done over all actions in an episode. Steps for Policy Gradient Algorithm: Initialize the Policy Network : For continuous actions, the network outputs parameters of a continuous distribution, usually gaussian, and thus, mean and variance. If there are multiple continuous inputs, then generate multiple distributions. For discrete actions, it outputs a probability distribution via softmax (a vector with the possibility of each action). Sample Trajectories : Use the current policy to interact with the environment. Collect states, actions, rewards, and log probabilities of chosen actions. Compute Discounted Rewards : For each step $t$, compute the discounted return: $$ R_t = \\sum_{k=t}^T \\gamma^{k-t} r_k $$ Here, $\\gamma$ is the discount factor, and $T$ is the episode length. Calculate the Loss : Use the loss function: $$ L(\\theta) = -\\frac{1}{N} \\sum_{t=1}^N \\log \\pi_\\theta(a_t | s_t) \\cdot R_t $$ This adjusts the policy to favor actions that lead to higher rewards. Update the Policy : Perform gradient descent on the loss $L(\\theta)$ to update the policy parameters $\\theta$. Implementation Please note that typically, policy gradient has worse performance under such a raw implementation. A better way would be to sample action space into discreet value then, use Q-learning. But policy gradient is the basis of many other methods, and so we still need to learn it. Here, a case with gravity is too complex for the model to handle, so we disabled the gravity, under which it yields acceptable result. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Normal import wandb from tqdm import trange device = \"cpu\" print(f\"Using device: {device}\") class PolicyNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(PolicyNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 64) self.fc2 = nn.Linear(64, 64) self.mean_layer = nn.Linear(64, action_dim) self.logstd_layer = nn.Linear(64, action_dim) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x) + x) mean = self.mean_layer(x) logstd = self.logstd_layer(x) return mean, logstd class PolicyGradientTrainer: def __init__(self, env, model, optimizer, gamma=0.99): self.env = env self.model = model self.optimizer = optimizer self.gamma = gamma self.episode_rewards = [] def compute_returns(self, rewards): discounted_rewards = [] running_return = 0 for r in reversed(rewards): running_return = r + self.gamma * running_return discounted_rewards.insert(0, running_return) return torch.tensor(discounted_rewards, dtype=torch.float32).to(device) def train(self, num_episodes=3000): epsilon = 0.9 epsilon_decay = 0.99 min_epsilon = 0.001 for episode in trange(num_episodes): state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) done = False truncated = False rewards = [] log_probs = [] cnt = 0 while not done and cnt < 200: cnt += 1 mean, logstd = self.model(state) std = torch.exp(logstd) dist = Normal(mean, std) action = dist.sample() log_prob = dist.log_prob(action).sum() action_clamped = torch.clamp(action, min=-2, max=2) next_state, reward, done, truncated, _ = self.env.step(action_clamped.cpu().numpy()) rewards.append(reward) log_probs.append(log_prob) state = torch.FloatTensor(next_state).to(device) if np.random.random() < epsilon: action = torch.FloatTensor(self.env.action_space.sample()).to(device) if len(rewards) == 0: continue returns = self.compute_returns(rewards) returns = (returns - returns.mean()) / (returns.std() + 1e-5) log_probs = torch.stack(log_probs) loss = -1e4 * torch.mean(log_probs * returns) self.optimizer.zero_grad() loss.backward() torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) self.optimizer.step() total_reward = sum(rewards) self.episode_rewards.append(total_reward) wandb.log({ \"episode\": episode, \"reward\": total_reward, \"loss\": loss.item(), \"mean_std\": std.mean().item() }) if episode % 100 == 0: print(f\"Episode {episode}, Reward: {total_reward:.1f}, Loss: {loss.item():.4f}, Mean Std: {std.mean().item():.4f}\") epsilon = max(epsilon * epsilon_decay, min_epsilon) def main(): wandb.init(project=\"rl\") env = gym.make(\"Pendulum-v1\", g=0) model = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(device) optimizer = optim.SGD(model.parameters(), lr=5e-3) trainer = PolicyGradientTrainer(env, model, optimizer, gamma=0.99) trainer.train() torch.save(model.state_dict(), \"policy_model.pth\") test(model) def test(model): env = gym.make(\"Pendulum-v1\", g=0, render_mode=\"human\") state, _ = env.reset() total_reward = 0 while True: with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) mean, logstd = model(state_tensor) action = torch.clamp(Normal(mean, torch.exp(logstd)).sample(), min=-2, max=2) next_state, reward, done, _, _ = env.step(action.cpu().numpy()) total_reward += reward state = next_state if done: break print(f\"Test Reward: {total_reward:.1f}\") env.close() if __name__ == \"__main__\": main()","title":"RL Ch2 Policy Gradient"},{"location":"02/#rl-ch2-policy-gradient","text":"","title":"RL Ch2 Policy Gradient"},{"location":"02/#policy-gradient","text":"Q-learning is simple but has its limitations. The major one being that, it only allows for discrete actions. For environments like MountainCarContinuous-v0 , it takes continuous acceleration value from negative one to positive one, and thus disallowing Q-learning. Policy gradient algorithm is still model-free, online-learning, and trial-and-error based, but it allows for continuous actions. Our model should give our a distribution of all the actions. That is, for continuous case, giving out the mean and variance for each input (assume we ust gaussian distribution), and for discreet case, giving out a softmax-ed vector. The major equation is the loss function: $$ L(\\theta) = -\\mathbb{E} \\big[ \\log \\pi_\\theta(a|s) \\cdot A \\big] $$ Here: - $\\pi_\\theta(a|s)$: The policy, which gives the probability of taking action $a$ in state $s$. - $A$: The advantage value. For now, it can equal to the cumulative reward for the given action. - The negative sign ensures we maximize rewards (since optimization minimizes the loss). - $\\mathbb{E}$ is done over all actions in an episode.","title":"Policy Gradient"},{"location":"02/#steps-for-policy-gradient-algorithm","text":"Initialize the Policy Network : For continuous actions, the network outputs parameters of a continuous distribution, usually gaussian, and thus, mean and variance. If there are multiple continuous inputs, then generate multiple distributions. For discrete actions, it outputs a probability distribution via softmax (a vector with the possibility of each action). Sample Trajectories : Use the current policy to interact with the environment. Collect states, actions, rewards, and log probabilities of chosen actions. Compute Discounted Rewards : For each step $t$, compute the discounted return: $$ R_t = \\sum_{k=t}^T \\gamma^{k-t} r_k $$ Here, $\\gamma$ is the discount factor, and $T$ is the episode length. Calculate the Loss : Use the loss function: $$ L(\\theta) = -\\frac{1}{N} \\sum_{t=1}^N \\log \\pi_\\theta(a_t | s_t) \\cdot R_t $$ This adjusts the policy to favor actions that lead to higher rewards. Update the Policy : Perform gradient descent on the loss $L(\\theta)$ to update the policy parameters $\\theta$.","title":"Steps for Policy Gradient Algorithm:"},{"location":"02/#implementation","text":"Please note that typically, policy gradient has worse performance under such a raw implementation. A better way would be to sample action space into discreet value then, use Q-learning. But policy gradient is the basis of many other methods, and so we still need to learn it. Here, a case with gravity is too complex for the model to handle, so we disabled the gravity, under which it yields acceptable result. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Normal import wandb from tqdm import trange device = \"cpu\" print(f\"Using device: {device}\") class PolicyNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(PolicyNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 64) self.fc2 = nn.Linear(64, 64) self.mean_layer = nn.Linear(64, action_dim) self.logstd_layer = nn.Linear(64, action_dim) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x) + x) mean = self.mean_layer(x) logstd = self.logstd_layer(x) return mean, logstd class PolicyGradientTrainer: def __init__(self, env, model, optimizer, gamma=0.99): self.env = env self.model = model self.optimizer = optimizer self.gamma = gamma self.episode_rewards = [] def compute_returns(self, rewards): discounted_rewards = [] running_return = 0 for r in reversed(rewards): running_return = r + self.gamma * running_return discounted_rewards.insert(0, running_return) return torch.tensor(discounted_rewards, dtype=torch.float32).to(device) def train(self, num_episodes=3000): epsilon = 0.9 epsilon_decay = 0.99 min_epsilon = 0.001 for episode in trange(num_episodes): state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) done = False truncated = False rewards = [] log_probs = [] cnt = 0 while not done and cnt < 200: cnt += 1 mean, logstd = self.model(state) std = torch.exp(logstd) dist = Normal(mean, std) action = dist.sample() log_prob = dist.log_prob(action).sum() action_clamped = torch.clamp(action, min=-2, max=2) next_state, reward, done, truncated, _ = self.env.step(action_clamped.cpu().numpy()) rewards.append(reward) log_probs.append(log_prob) state = torch.FloatTensor(next_state).to(device) if np.random.random() < epsilon: action = torch.FloatTensor(self.env.action_space.sample()).to(device) if len(rewards) == 0: continue returns = self.compute_returns(rewards) returns = (returns - returns.mean()) / (returns.std() + 1e-5) log_probs = torch.stack(log_probs) loss = -1e4 * torch.mean(log_probs * returns) self.optimizer.zero_grad() loss.backward() torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) self.optimizer.step() total_reward = sum(rewards) self.episode_rewards.append(total_reward) wandb.log({ \"episode\": episode, \"reward\": total_reward, \"loss\": loss.item(), \"mean_std\": std.mean().item() }) if episode % 100 == 0: print(f\"Episode {episode}, Reward: {total_reward:.1f}, Loss: {loss.item():.4f}, Mean Std: {std.mean().item():.4f}\") epsilon = max(epsilon * epsilon_decay, min_epsilon) def main(): wandb.init(project=\"rl\") env = gym.make(\"Pendulum-v1\", g=0) model = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(device) optimizer = optim.SGD(model.parameters(), lr=5e-3) trainer = PolicyGradientTrainer(env, model, optimizer, gamma=0.99) trainer.train() torch.save(model.state_dict(), \"policy_model.pth\") test(model) def test(model): env = gym.make(\"Pendulum-v1\", g=0, render_mode=\"human\") state, _ = env.reset() total_reward = 0 while True: with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) mean, logstd = model(state_tensor) action = torch.clamp(Normal(mean, torch.exp(logstd)).sample(), min=-2, max=2) next_state, reward, done, _, _ = env.step(action.cpu().numpy()) total_reward += reward state = next_state if done: break print(f\"Test Reward: {total_reward:.1f}\") env.close() if __name__ == \"__main__\": main()","title":"Implementation"},{"location":"03/","text":"RL Ch3 Actor-Critic Actor-Critic Actor-Critic is an algorithm that uses two models. Actor , the policy network, is responsible for selecting actions given a state, while Critic , the value network, evaluates the actions taken by the Actor by estimating the value function. This combination allows the algorithm to benefit from both policy-based and value-based methods. Actor The Actor represents the policy $\\pi(a|s)$, which is usually parameterized by a neural network. Given a state $s$, the network outputs a probability distribution over actions, or parameters of a distribution (for example, mean and standard deviation in the case of a Gaussian policy). The objective is to maximize the expected return: $$ J(\\pi) = \\mathbb{E}_{\\pi}\\left[ R \\right], $$ where $R$ is the cumulative reward. The policy parameters $\\theta$ are updated using gradient ascent based on the policy gradient: $$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E} {\\pi {\\theta}}\\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) \\right] $$ with $Q^{\\pi}(s, a)$ representing the action-value function. This is identical to the policy gradient method, using the value given out by the critic network as the advantage function. Critic The Critic estimates the value function, which can be either the state-value function $V(s)$ or the action-value function $Q(s, a)$. Its main role is to provide feedback to the Actor regarding the quality of actions taken. The Critic minimizes the mean squared error, $$ L = (r + \\gamma V(s') - V(s)) ^ 2 $$ where: $r$ is the immediate reward, $\\gamma$ is the discount factor, $s'$ is the next state. updating its parameters $\\phi$ accordingly. Of course, if you use $Q(s, a)$, then the Critic model is identical to Q-learning. That is, instead of estimating the state-value function $V(s)$, the Critic estimates the action-value function directly. In this case, the Critic is trained to minimize the loss, $$ L = (r + \\gamma \\max_{a'} Q(s', a'), - Q(s, a)) ^ 2 $$ Combing Actor and Critic In an Actor-Critic framework, both the Actor and the Critic are trained simultaneously and interact with each other during the learning process. Here\u2019s how the two models work together: Action Selection: At time step $t$, given the current state $s_t$, the Actor selects an action $a_t$ according to its policy $\\pi_{\\theta}(a_t|s_t)$. Environment Interaction: The selected action $a_t$ is executed in the environment, which then returns a reward $r_t$ and the next state $s_{t+1}$. Critic Evaluation: The Critic evaluates the quality of the action by estimating the value function. This can be either the state-value $V(s_t)$ or the action-value $Q(s_t,a_t)$. If using $V(s)$, the Temporal Difference (TD) error (also known as GAE generalized advantage estimation) is computed as: $$ \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t). $$ If using $Q(s,a)$, a similar TD error (or Bellman error) is computed based on the Q-learning target: $$ \\delta_t = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t,a_t). $$ Critic Update: The Critic\u2019s parameters $\\phi$ are updated to minimize the squared TD error: $$ L(\\phi) = \\delta_t^2. $$ The gradient descent update for the Critic is: $$ \\phi \\leftarrow \\phi - \\beta \\nabla_{\\phi} \\left( \\delta_t^2 \\right), $$ where $\\beta$ is the learning rate for the Critic. Actor Update: The Actor is updated using the policy gradient method, with the advantage (previously, reward, but now, the value) serving as the weight. The update rule for the Actor\u2019s parameters $\\theta$ is: $$ \\theta \\leftarrow \\theta + \\alpha \\, \\delta_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t), $$ where $\\alpha$ is the Actor\u2019s learning rate. This update increases the probability of actions that yield a positive advantage (i.e., better-than-expected outcomes) and decreases it for actions with a negative advantage. Implementation Using actor-critic algorithm leads to faster convergence. However, the task is still too complicated with gravity. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Normal import wandb from tqdm import trange device = \"cpu\" print(f\"Using device: {device}\") class ActorNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(ActorNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.mean_layer = nn.Linear(128, action_dim) self.logstd_layer = nn.Linear(128, action_dim) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) mean = self.mean_layer(x) logstd = self.logstd_layer(x) logstd = torch.clamp(logstd, min=-20, max=2) return mean, logstd class CriticNetwork(nn.Module): def __init__(self, state_dim): super(CriticNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.value_layer = nn.Linear(128, 1) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) value = self.value_layer(x) return value class ActorCriticTrainer: def __init__(self, env, actor_model, critic_model, actor_optimizer, critic_optimizer, gamma=0.99): self.env = env self.actor_model = actor_model self.critic_model = critic_model self.actor_optimizer = actor_optimizer self.critic_optimizer = critic_optimizer self.gamma = gamma self.episode_rewards = [] def compute_returns(self, rewards): discounted_rewards = [] running_return = 0 for r in reversed(rewards): running_return = r + self.gamma * running_return discounted_rewards.insert(0, running_return) return torch.tensor(discounted_rewards, dtype=torch.float32).to(device) def train(self, num_episodes=5000): for episode in trange(num_episodes): state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) done = False truncated = False rewards = [] log_probs = [] values = [] while not done and not truncated: mean, logstd = self.actor_model(state) std = torch.exp(logstd) dist = Normal(mean, std) action = dist.sample() log_prob = dist.log_prob(action).sum() action_clamped = torch.clamp(action, min=-2, max=2) value = self.critic_model(state) next_state, reward, done, truncated, _ = self.env.step(action_clamped.cpu().numpy()) rewards.append(reward) log_probs.append(log_prob) values.append(value.squeeze()) state = torch.FloatTensor(next_state).to(device) if len(rewards) == 0: continue returns = self.compute_returns(rewards) values = torch.stack(values) advantages = returns - values.detach() advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) log_probs = torch.stack(log_probs) actor_loss = -torch.mean(log_probs * advantages) critic_loss = nn.MSELoss()(values, returns) self.critic_optimizer.zero_grad() critic_loss.backward() torch.nn.utils.clip_grad_norm_(self.critic_model.parameters(), 1.0) self.critic_optimizer.step() self.actor_optimizer.zero_grad() actor_loss.backward() torch.nn.utils.clip_grad_norm_(self.actor_model.parameters(), 1.0) self.actor_optimizer.step() total_reward = sum(rewards) self.episode_rewards.append(total_reward) wandb.log({ \"episode\": episode, \"reward\": total_reward, \"actor_loss\": actor_loss.item(), \"critic_loss\": critic_loss.item(), \"mean_std\": std.mean().item() }) if episode % 100 == 0: print(f\"Episode {episode}, Reward: {total_reward:.1f}\") def main(): wandb.init(project=\"rl-fixed\") env = gym.make(\"Pendulum-v1\", g=0) state_dim = env.observation_space.shape[0] action_dim = env.action_space.shape[0] actor_model = ActorNetwork(state_dim, action_dim).to(device) critic_model = CriticNetwork(state_dim).to(device) actor_optimizer = optim.Adam(actor_model.parameters(), lr=3e-4) critic_optimizer = optim.Adam(critic_model.parameters(), lr=1e-3) trainer = ActorCriticTrainer(env, actor_model, critic_model, actor_optimizer, critic_optimizer, gamma=0.99) trainer.train() torch.save(actor_model.state_dict(), \"actor_fixed.pth\") torch.save(critic_model.state_dict(), \"critic_fixed.pth\") test(actor_model) def test(actor_model): env = gym.make(\"Pendulum-v1\", render_mode=\"human\", g=0) state, _ = env.reset() total_reward = 0 while True: with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) mean, logstd = actor_model(state_tensor) action = torch.clamp(Normal(mean, torch.exp(logstd)).sample(), min=-2, max=2) next_state, reward, done, _, _ = env.step(action.cpu().numpy()) total_reward += reward state = next_state if done: break print(f\"Test Reward: {total_reward:.1f}\") env.close() if __name__ == \"__main__\": main()","title":"RL Ch3 Actor-Critic"},{"location":"03/#rl-ch3-actor-critic","text":"","title":"RL Ch3 Actor-Critic"},{"location":"03/#actor-critic","text":"Actor-Critic is an algorithm that uses two models. Actor , the policy network, is responsible for selecting actions given a state, while Critic , the value network, evaluates the actions taken by the Actor by estimating the value function. This combination allows the algorithm to benefit from both policy-based and value-based methods.","title":"Actor-Critic"},{"location":"03/#actor","text":"The Actor represents the policy $\\pi(a|s)$, which is usually parameterized by a neural network. Given a state $s$, the network outputs a probability distribution over actions, or parameters of a distribution (for example, mean and standard deviation in the case of a Gaussian policy). The objective is to maximize the expected return: $$ J(\\pi) = \\mathbb{E}_{\\pi}\\left[ R \\right], $$ where $R$ is the cumulative reward. The policy parameters $\\theta$ are updated using gradient ascent based on the policy gradient: $$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E} {\\pi {\\theta}}\\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) \\right] $$ with $Q^{\\pi}(s, a)$ representing the action-value function. This is identical to the policy gradient method, using the value given out by the critic network as the advantage function.","title":"Actor"},{"location":"03/#critic","text":"The Critic estimates the value function, which can be either the state-value function $V(s)$ or the action-value function $Q(s, a)$. Its main role is to provide feedback to the Actor regarding the quality of actions taken. The Critic minimizes the mean squared error, $$ L = (r + \\gamma V(s') - V(s)) ^ 2 $$ where: $r$ is the immediate reward, $\\gamma$ is the discount factor, $s'$ is the next state. updating its parameters $\\phi$ accordingly. Of course, if you use $Q(s, a)$, then the Critic model is identical to Q-learning. That is, instead of estimating the state-value function $V(s)$, the Critic estimates the action-value function directly. In this case, the Critic is trained to minimize the loss, $$ L = (r + \\gamma \\max_{a'} Q(s', a'), - Q(s, a)) ^ 2 $$","title":"Critic"},{"location":"03/#combing-actor-and-critic","text":"In an Actor-Critic framework, both the Actor and the Critic are trained simultaneously and interact with each other during the learning process. Here\u2019s how the two models work together: Action Selection: At time step $t$, given the current state $s_t$, the Actor selects an action $a_t$ according to its policy $\\pi_{\\theta}(a_t|s_t)$. Environment Interaction: The selected action $a_t$ is executed in the environment, which then returns a reward $r_t$ and the next state $s_{t+1}$. Critic Evaluation: The Critic evaluates the quality of the action by estimating the value function. This can be either the state-value $V(s_t)$ or the action-value $Q(s_t,a_t)$. If using $V(s)$, the Temporal Difference (TD) error (also known as GAE generalized advantage estimation) is computed as: $$ \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t). $$ If using $Q(s,a)$, a similar TD error (or Bellman error) is computed based on the Q-learning target: $$ \\delta_t = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t,a_t). $$ Critic Update: The Critic\u2019s parameters $\\phi$ are updated to minimize the squared TD error: $$ L(\\phi) = \\delta_t^2. $$ The gradient descent update for the Critic is: $$ \\phi \\leftarrow \\phi - \\beta \\nabla_{\\phi} \\left( \\delta_t^2 \\right), $$ where $\\beta$ is the learning rate for the Critic. Actor Update: The Actor is updated using the policy gradient method, with the advantage (previously, reward, but now, the value) serving as the weight. The update rule for the Actor\u2019s parameters $\\theta$ is: $$ \\theta \\leftarrow \\theta + \\alpha \\, \\delta_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t), $$ where $\\alpha$ is the Actor\u2019s learning rate. This update increases the probability of actions that yield a positive advantage (i.e., better-than-expected outcomes) and decreases it for actions with a negative advantage.","title":"Combing Actor and Critic"},{"location":"03/#implementation","text":"Using actor-critic algorithm leads to faster convergence. However, the task is still too complicated with gravity. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Normal import wandb from tqdm import trange device = \"cpu\" print(f\"Using device: {device}\") class ActorNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(ActorNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.mean_layer = nn.Linear(128, action_dim) self.logstd_layer = nn.Linear(128, action_dim) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) mean = self.mean_layer(x) logstd = self.logstd_layer(x) logstd = torch.clamp(logstd, min=-20, max=2) return mean, logstd class CriticNetwork(nn.Module): def __init__(self, state_dim): super(CriticNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.value_layer = nn.Linear(128, 1) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) value = self.value_layer(x) return value class ActorCriticTrainer: def __init__(self, env, actor_model, critic_model, actor_optimizer, critic_optimizer, gamma=0.99): self.env = env self.actor_model = actor_model self.critic_model = critic_model self.actor_optimizer = actor_optimizer self.critic_optimizer = critic_optimizer self.gamma = gamma self.episode_rewards = [] def compute_returns(self, rewards): discounted_rewards = [] running_return = 0 for r in reversed(rewards): running_return = r + self.gamma * running_return discounted_rewards.insert(0, running_return) return torch.tensor(discounted_rewards, dtype=torch.float32).to(device) def train(self, num_episodes=5000): for episode in trange(num_episodes): state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) done = False truncated = False rewards = [] log_probs = [] values = [] while not done and not truncated: mean, logstd = self.actor_model(state) std = torch.exp(logstd) dist = Normal(mean, std) action = dist.sample() log_prob = dist.log_prob(action).sum() action_clamped = torch.clamp(action, min=-2, max=2) value = self.critic_model(state) next_state, reward, done, truncated, _ = self.env.step(action_clamped.cpu().numpy()) rewards.append(reward) log_probs.append(log_prob) values.append(value.squeeze()) state = torch.FloatTensor(next_state).to(device) if len(rewards) == 0: continue returns = self.compute_returns(rewards) values = torch.stack(values) advantages = returns - values.detach() advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) log_probs = torch.stack(log_probs) actor_loss = -torch.mean(log_probs * advantages) critic_loss = nn.MSELoss()(values, returns) self.critic_optimizer.zero_grad() critic_loss.backward() torch.nn.utils.clip_grad_norm_(self.critic_model.parameters(), 1.0) self.critic_optimizer.step() self.actor_optimizer.zero_grad() actor_loss.backward() torch.nn.utils.clip_grad_norm_(self.actor_model.parameters(), 1.0) self.actor_optimizer.step() total_reward = sum(rewards) self.episode_rewards.append(total_reward) wandb.log({ \"episode\": episode, \"reward\": total_reward, \"actor_loss\": actor_loss.item(), \"critic_loss\": critic_loss.item(), \"mean_std\": std.mean().item() }) if episode % 100 == 0: print(f\"Episode {episode}, Reward: {total_reward:.1f}\") def main(): wandb.init(project=\"rl-fixed\") env = gym.make(\"Pendulum-v1\", g=0) state_dim = env.observation_space.shape[0] action_dim = env.action_space.shape[0] actor_model = ActorNetwork(state_dim, action_dim).to(device) critic_model = CriticNetwork(state_dim).to(device) actor_optimizer = optim.Adam(actor_model.parameters(), lr=3e-4) critic_optimizer = optim.Adam(critic_model.parameters(), lr=1e-3) trainer = ActorCriticTrainer(env, actor_model, critic_model, actor_optimizer, critic_optimizer, gamma=0.99) trainer.train() torch.save(actor_model.state_dict(), \"actor_fixed.pth\") torch.save(critic_model.state_dict(), \"critic_fixed.pth\") test(actor_model) def test(actor_model): env = gym.make(\"Pendulum-v1\", render_mode=\"human\", g=0) state, _ = env.reset() total_reward = 0 while True: with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) mean, logstd = actor_model(state_tensor) action = torch.clamp(Normal(mean, torch.exp(logstd)).sample(), min=-2, max=2) next_state, reward, done, _, _ = env.step(action.cpu().numpy()) total_reward += reward state = next_state if done: break print(f\"Test Reward: {total_reward:.1f}\") env.close() if __name__ == \"__main__\": main()","title":"Implementation"},{"location":"04/","text":"RL Ch4 PPO PPO Actor-Critic improves the performance of a policy network, and there are more ways to do so. Another improvement based on Actor-Critic is PPO (Proximal Policy Optimization). PPO is designed to optimize the policy more efficiently by introducing a constraint to ensure that each policy update does not change the policy too drastically. This is done by clipping the objective function to prevent large, unstable updates, while still allowing for enough flexibility for effective learning. A Recapitulation We would use the following loss for a typical policy network. $$ L = -\\mathbb{E}(\\log(\\pi_\\theta(a_t|s_t)) A) $$ Where $A$ is the advantage. In the vanilla policy gradient, this equals the reward, and in the actor-critic architecture, it is the value given out by the critic network. PPO differentiates from this method by introducing a clipped objective that prevents excessively large policy updates. The idea is to ensure the new policy doesn't deviate too far from the old policy, even if the objective function suggests a large update. The Clipped Surrogate Objective The key modification in PPO is the use of a clipped surrogate objective to prevent large updates. The clipped objective is defined as: $$ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] $$ Where: - $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$ is the probability ratio between the new policy and the old policy. - $\\hat{A}_t$ is the advantage estimate at time step $t$. - $\\epsilon$ is a hyperparameter that defines the clipping range (usually small, such as 0.1 or 0.2). - $\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)$ is the clipped ratio that limits how much the probability ratio can change. This modification ensures that if the new policy makes a drastic change, the objective function will be \"penalized\" by the clipping, discouraging large updates. On the other hand, if the change is small (i.e., the ratio $r_t(\\theta)$ is within the clipping bounds), the objective behaves as a standard policy gradient loss. How to Obtain $r_t(\\theta)$ ? A engineering issue here is that how to get the old distribution. We only need to firstly, run multiple inferences in a batch without gradient, then for each entry in the batch, update the model. For the first entry, the $r_t(\\theta)$ would stay as one, but for the later ones, it's value is correct. Entropy Regulation A common trick for policy gradient learning is to increase the loss by $\\alpha H$. Where $\\alpha$ ia a small number, typically $0.01$. And $H$ is the entropy. The entropy term measures the randomness or uncertainty of the policy. A higher entropy means the policy is more stochastic, encouraging the agent to explore a wider range of actions. On the other hand, a lower entropy indicates a more deterministic policy. The entropy term is defined as: $$ H(\\pi) = -\\mathbb{E}(\\pi log(\\pi)) $$ This term quantifies how uncertain the policy is about which action to choose in a given state. In reinforcement learning, encouraging exploration by increasing entropy can prevent the agent from converging to suboptimal deterministic policies too early, ensuring that it continues to explore different actions to discover better strategies. Implementation PPO yields excellent stability and convergence, so we enable the gravity for this model. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Normal import wandb from tqdm import trange device = \"cpu\" print(f\"Using device: {device}\") class ActorNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(ActorNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.mean_layer = nn.Linear(128, action_dim) self.logstd_layer = nn.Linear(128, action_dim) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) mean = self.mean_layer(x) logstd = self.logstd_layer(x) logstd = torch.clamp(logstd, min=-20, max=2) return mean, logstd class CriticNetwork(nn.Module): def __init__(self, state_dim): super(CriticNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.value_layer = nn.Linear(128, 1) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) value = self.value_layer(x) return value class PPOTrainer: def __init__(self, env, actor_model, critic_model, actor_optimizer, critic_optimizer, gamma=0.99, gae_lambda=0.95, clip_param=0.2, n_epochs=4, batch_size=64): self.env = env self.actor_model = actor_model self.critic_model = critic_model self.actor_optimizer = actor_optimizer self.critic_optimizer = critic_optimizer self.gamma = gamma self.gae_lambda = gae_lambda self.clip_param = clip_param self.n_epochs = n_epochs self.batch_size = batch_size self.episode_rewards = [] def compute_gae(self, rewards, values, dones, next_value): values = values + [next_value] advantages = [] gae = 0 for t in reversed(range(len(rewards))): delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t] gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae advantages.insert(0, gae) return torch.tensor(advantages, dtype=torch.float32).to(device) def train(self, num_updates=1000, n_steps=2048): for update in trange(num_updates): states, actions, rewards, dones, old_log_probs, values = [], [], [], [], [], [] state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) episode_reward = 0 for _ in range(n_steps): with torch.no_grad(): mean, logstd = self.actor_model(state) std = torch.exp(logstd) dist = Normal(mean, std) action = dist.sample() log_prob = dist.log_prob(action).sum() value = self.critic_model(state).squeeze() next_state, reward, done, truncated, _ = self.env.step(action.cpu().numpy()) episode_reward += reward states.append(state) actions.append(action) rewards.append(reward) dones.append(done or truncated) old_log_probs.append(log_prob) values.append(value) state = torch.FloatTensor(next_state).to(device) if not (done or truncated) else torch.FloatTensor(self.env.reset()[0]).to(device) if done or truncated: self.episode_rewards.append(episode_reward) episode_reward = 0 with torch.no_grad(): next_value = self.critic_model(state).squeeze().item() states = torch.stack(states) actions = torch.stack(actions) old_log_probs = torch.stack(old_log_probs) values = torch.stack(values).cpu().numpy() advantages = self.compute_gae(rewards, values.tolist(), dones, next_value) returns = advantages + torch.tensor(values, dtype=torch.float32).to(device) advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) dataset = torch.utils.data.TensorDataset(states, actions, old_log_probs, returns, advantages) dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True) for _ in range(self.n_epochs): for batch in dataloader: b_states, b_actions, b_old_log_probs, b_returns, b_advantages = batch mean, logstd = self.actor_model(b_states) std = torch.exp(logstd) dist = Normal(mean, std) new_log_probs = dist.log_prob(b_actions).sum(dim=1) entropy = dist.entropy().mean() ratio = torch.exp(new_log_probs - b_old_log_probs) surr1 = ratio * b_advantages surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * b_advantages actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy current_values = self.critic_model(b_states).squeeze() critic_loss = nn.MSELoss()(current_values, b_returns) self.actor_optimizer.zero_grad() actor_loss.backward() torch.nn.utils.clip_grad_norm_(self.actor_model.parameters(), 0.5) self.actor_optimizer.step() self.critic_optimizer.zero_grad() critic_loss.backward() torch.nn.utils.clip_grad_norm_(self.critic_model.parameters(), 0.5) self.critic_optimizer.step() if len(self.episode_rewards) > 0: avg_reward = np.mean(self.episode_rewards[-10:]) wandb.log({ \"update\": update, \"avg_reward\": avg_reward, \"actor_loss\": actor_loss.item(), \"critic_loss\": critic_loss.item(), \"entropy\": entropy.item() }) if update % 10 == 0: print(f\"Update {update}, Avg Reward: {avg_reward:.1f}\") def main(): wandb.init(project=\"rl\") env = gym.make(\"Pendulum-v1\") state_dim = env.observation_space.shape[0] action_dim = env.action_space.shape[0] actor_model = ActorNetwork(state_dim, action_dim).to(device) critic_model = CriticNetwork(state_dim).to(device) actor_optimizer = optim.Adam(actor_model.parameters(), lr=3e-4) critic_optimizer = optim.Adam(critic_model.parameters(), lr=3e-4) trainer = PPOTrainer( env=env, actor_model=actor_model, critic_model=critic_model, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer, gamma=0.99, gae_lambda=0.95, clip_param=0.2, n_epochs=4, batch_size=64 ) trainer.train(num_updates=500, n_steps=2048) torch.save(actor_model.state_dict(), \"ppo_actor.pth\") torch.save(critic_model.state_dict(), \"ppo_critic.pth\") test(actor_model) def test(actor_model): env = gym.make(\"Pendulum-v1\", render_mode=\"human\") state, _ = env.reset() total_reward = 0 while True: with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) mean, logstd = actor_model(state_tensor) action = torch.clamp(Normal(mean, torch.exp(logstd)).sample(), min=-2, max=2) next_state, reward, done, _, _ = env.step(action.cpu().numpy()) total_reward += reward state = next_state if done: break print(f\"Test Reward: {total_reward:.1f}\") env.close() if __name__ == \"__main__\": main()","title":"RL Ch4 PPO"},{"location":"04/#rl-ch4-ppo","text":"","title":"RL Ch4 PPO"},{"location":"04/#ppo","text":"Actor-Critic improves the performance of a policy network, and there are more ways to do so. Another improvement based on Actor-Critic is PPO (Proximal Policy Optimization). PPO is designed to optimize the policy more efficiently by introducing a constraint to ensure that each policy update does not change the policy too drastically. This is done by clipping the objective function to prevent large, unstable updates, while still allowing for enough flexibility for effective learning.","title":"PPO"},{"location":"04/#a-recapitulation","text":"We would use the following loss for a typical policy network. $$ L = -\\mathbb{E}(\\log(\\pi_\\theta(a_t|s_t)) A) $$ Where $A$ is the advantage. In the vanilla policy gradient, this equals the reward, and in the actor-critic architecture, it is the value given out by the critic network. PPO differentiates from this method by introducing a clipped objective that prevents excessively large policy updates. The idea is to ensure the new policy doesn't deviate too far from the old policy, even if the objective function suggests a large update.","title":"A Recapitulation"},{"location":"04/#the-clipped-surrogate-objective","text":"The key modification in PPO is the use of a clipped surrogate objective to prevent large updates. The clipped objective is defined as: $$ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] $$ Where: - $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$ is the probability ratio between the new policy and the old policy. - $\\hat{A}_t$ is the advantage estimate at time step $t$. - $\\epsilon$ is a hyperparameter that defines the clipping range (usually small, such as 0.1 or 0.2). - $\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)$ is the clipped ratio that limits how much the probability ratio can change. This modification ensures that if the new policy makes a drastic change, the objective function will be \"penalized\" by the clipping, discouraging large updates. On the other hand, if the change is small (i.e., the ratio $r_t(\\theta)$ is within the clipping bounds), the objective behaves as a standard policy gradient loss.","title":"The Clipped Surrogate Objective"},{"location":"04/#how-to-obtain-r_ttheta","text":"A engineering issue here is that how to get the old distribution. We only need to firstly, run multiple inferences in a batch without gradient, then for each entry in the batch, update the model. For the first entry, the $r_t(\\theta)$ would stay as one, but for the later ones, it's value is correct.","title":"How to Obtain $r_t(\\theta)$ ?"},{"location":"04/#entropy-regulation","text":"A common trick for policy gradient learning is to increase the loss by $\\alpha H$. Where $\\alpha$ ia a small number, typically $0.01$. And $H$ is the entropy. The entropy term measures the randomness or uncertainty of the policy. A higher entropy means the policy is more stochastic, encouraging the agent to explore a wider range of actions. On the other hand, a lower entropy indicates a more deterministic policy. The entropy term is defined as: $$ H(\\pi) = -\\mathbb{E}(\\pi log(\\pi)) $$ This term quantifies how uncertain the policy is about which action to choose in a given state. In reinforcement learning, encouraging exploration by increasing entropy can prevent the agent from converging to suboptimal deterministic policies too early, ensuring that it continues to explore different actions to discover better strategies.","title":"Entropy Regulation"},{"location":"04/#implementation","text":"PPO yields excellent stability and convergence, so we enable the gravity for this model. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Normal import wandb from tqdm import trange device = \"cpu\" print(f\"Using device: {device}\") class ActorNetwork(nn.Module): def __init__(self, state_dim, action_dim): super(ActorNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.mean_layer = nn.Linear(128, action_dim) self.logstd_layer = nn.Linear(128, action_dim) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) mean = self.mean_layer(x) logstd = self.logstd_layer(x) logstd = torch.clamp(logstd, min=-20, max=2) return mean, logstd class CriticNetwork(nn.Module): def __init__(self, state_dim): super(CriticNetwork, self).__init__() self.fc1 = nn.Linear(state_dim, 128) self.fc2 = nn.Linear(128, 128) self.value_layer = nn.Linear(128, 1) self.apply(self.init_weights) @staticmethod def init_weights(m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=np.sqrt(2)) nn.init.constant_(m.bias, 0.0) def forward(self, state): x = torch.relu(self.fc1(state)) x = torch.relu(self.fc2(x)) value = self.value_layer(x) return value class PPOTrainer: def __init__(self, env, actor_model, critic_model, actor_optimizer, critic_optimizer, gamma=0.99, gae_lambda=0.95, clip_param=0.2, n_epochs=4, batch_size=64): self.env = env self.actor_model = actor_model self.critic_model = critic_model self.actor_optimizer = actor_optimizer self.critic_optimizer = critic_optimizer self.gamma = gamma self.gae_lambda = gae_lambda self.clip_param = clip_param self.n_epochs = n_epochs self.batch_size = batch_size self.episode_rewards = [] def compute_gae(self, rewards, values, dones, next_value): values = values + [next_value] advantages = [] gae = 0 for t in reversed(range(len(rewards))): delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t] gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae advantages.insert(0, gae) return torch.tensor(advantages, dtype=torch.float32).to(device) def train(self, num_updates=1000, n_steps=2048): for update in trange(num_updates): states, actions, rewards, dones, old_log_probs, values = [], [], [], [], [], [] state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) episode_reward = 0 for _ in range(n_steps): with torch.no_grad(): mean, logstd = self.actor_model(state) std = torch.exp(logstd) dist = Normal(mean, std) action = dist.sample() log_prob = dist.log_prob(action).sum() value = self.critic_model(state).squeeze() next_state, reward, done, truncated, _ = self.env.step(action.cpu().numpy()) episode_reward += reward states.append(state) actions.append(action) rewards.append(reward) dones.append(done or truncated) old_log_probs.append(log_prob) values.append(value) state = torch.FloatTensor(next_state).to(device) if not (done or truncated) else torch.FloatTensor(self.env.reset()[0]).to(device) if done or truncated: self.episode_rewards.append(episode_reward) episode_reward = 0 with torch.no_grad(): next_value = self.critic_model(state).squeeze().item() states = torch.stack(states) actions = torch.stack(actions) old_log_probs = torch.stack(old_log_probs) values = torch.stack(values).cpu().numpy() advantages = self.compute_gae(rewards, values.tolist(), dones, next_value) returns = advantages + torch.tensor(values, dtype=torch.float32).to(device) advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) dataset = torch.utils.data.TensorDataset(states, actions, old_log_probs, returns, advantages) dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True) for _ in range(self.n_epochs): for batch in dataloader: b_states, b_actions, b_old_log_probs, b_returns, b_advantages = batch mean, logstd = self.actor_model(b_states) std = torch.exp(logstd) dist = Normal(mean, std) new_log_probs = dist.log_prob(b_actions).sum(dim=1) entropy = dist.entropy().mean() ratio = torch.exp(new_log_probs - b_old_log_probs) surr1 = ratio * b_advantages surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * b_advantages actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy current_values = self.critic_model(b_states).squeeze() critic_loss = nn.MSELoss()(current_values, b_returns) self.actor_optimizer.zero_grad() actor_loss.backward() torch.nn.utils.clip_grad_norm_(self.actor_model.parameters(), 0.5) self.actor_optimizer.step() self.critic_optimizer.zero_grad() critic_loss.backward() torch.nn.utils.clip_grad_norm_(self.critic_model.parameters(), 0.5) self.critic_optimizer.step() if len(self.episode_rewards) > 0: avg_reward = np.mean(self.episode_rewards[-10:]) wandb.log({ \"update\": update, \"avg_reward\": avg_reward, \"actor_loss\": actor_loss.item(), \"critic_loss\": critic_loss.item(), \"entropy\": entropy.item() }) if update % 10 == 0: print(f\"Update {update}, Avg Reward: {avg_reward:.1f}\") def main(): wandb.init(project=\"rl\") env = gym.make(\"Pendulum-v1\") state_dim = env.observation_space.shape[0] action_dim = env.action_space.shape[0] actor_model = ActorNetwork(state_dim, action_dim).to(device) critic_model = CriticNetwork(state_dim).to(device) actor_optimizer = optim.Adam(actor_model.parameters(), lr=3e-4) critic_optimizer = optim.Adam(critic_model.parameters(), lr=3e-4) trainer = PPOTrainer( env=env, actor_model=actor_model, critic_model=critic_model, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer, gamma=0.99, gae_lambda=0.95, clip_param=0.2, n_epochs=4, batch_size=64 ) trainer.train(num_updates=500, n_steps=2048) torch.save(actor_model.state_dict(), \"ppo_actor.pth\") torch.save(critic_model.state_dict(), \"ppo_critic.pth\") test(actor_model) def test(actor_model): env = gym.make(\"Pendulum-v1\", render_mode=\"human\") state, _ = env.reset() total_reward = 0 while True: with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) mean, logstd = actor_model(state_tensor) action = torch.clamp(Normal(mean, torch.exp(logstd)).sample(), min=-2, max=2) next_state, reward, done, _, _ = env.step(action.cpu().numpy()) total_reward += reward state = next_state if done: break print(f\"Test Reward: {total_reward:.1f}\") env.close() if __name__ == \"__main__\": main()","title":"Implementation"},{"location":"05/","text":"RL Ch5 GRPO Introduction to GRPO GRPO (Group Relative Policy Optimization) is a novel reinforcement learning method proposed by DeepSeek, specifically designed for large language model (LLM) reinforcement learning. It builds upon the ideas of PPO (Proximal Policy Optimization) but eliminates the need for a critic model, simplifying the overall architecture. Core Concept of GRPO The core idea of GRPO is to optimize the policy by comparing the outputs of the new policy with a group of outputs from the old policy. This \"group relative\" comparison ensures that the updates are more stable and reliable. Instead of relying on a value function to estimate the advantage, GRPO uses relative rewards computed from multiple samples. Objective Function of GRPO The objective function of GRPO can be formulated as: $$ L^{GRPO}(\\theta) = \\mathbb{E}\\left[ \\sum_{i = 1}^{G}\\left(\\min \\left(\\frac{\\pi_{\\theta}\\left(o_{i}\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(o_{i}\\right)} A_{i}, \\text{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{i}\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(o_{i}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right)\\right) - \\beta \\mathbb{D} {KL}\\left(\\pi {\\theta} | \\pi_{\\text{ref}}\\right) \\right] $$ Where: - $A_{i} = \\frac{r_{i} - \\mathrm{mean}({r_1, r_2, \\cdots, r_G})}{\\mathrm{std}({r_1, r_2, \\cdots, r_G})}$ is the normalized advantage computed from a group of samples. Please note that, in GRPO, we use this group-normalized reward instead of GAE. The latter requires a critic model, which we do not have in GRPO. - $\\pi_{\\theta}$ and $\\pi_{\\theta_{\\text{old}}}$ are the new and old policies, respectively. - $\\varepsilon$ is a hyperparameter that controls the range of policy updates. - $\\beta$ is a regularization parameter that controls the KL divergence between the new policy and a reference policy $\\pi_{\\text{ref}}$. This functions the same as the entropy regulation trick we talked about in the last chapter, it is just that GRPO uses KL divergence instead of simple entropy (KL divergence is just entropy difference). Advantages of GRPO No Critic Network Required : By eliminating the need for a critic network, GRPO simplifies the model architecture and reduces computational overhead. Stable Updates : The clipping mechanism ensures that policy updates are stable and prevent large, potentially destabilizing changes. Robustness to Noisy Rewards : GRPO is more robust to noisy reward signals, making it suitable for complex tasks where reward estimation can be challenging. Implementation GRPO does not behave as well as PPO, so we use a simpler scenario. At the same time, this also demonstrates how to use policy gradient on discreet action spaces. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Categorical import wandb from tqdm import trange device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") class ActorNetwork(nn.Module): def __init__(self, state_dim, action_dim): super().__init__() self.net = nn.Sequential( nn.Linear(state_dim, 64), nn.Tanh(), nn.Linear(64, 64), nn.Tanh(), nn.Linear(64, action_dim) ) self.apply(self._init_weights) def _init_weights(self, m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=1.0) nn.init.constant_(m.bias, 0.0) def forward(self, x): return self.net(x) class GRPOTrainer: def __init__(self, env, actor, optimizer, clip_ratio=0.2, beta=0.001, gamma=0.99, epochs=10, batch_size=32, group_size=200): self.env = env self.actor = actor self.optimizer = optimizer self.clip_ratio = clip_ratio self.beta = beta self.gamma = gamma self.epochs = epochs self.batch_size = batch_size self.group_size = group_size self.ep_rewards = [] def _calc_returns(self, rewards, dones): returns = [] R = 0 for r, done in zip(reversed(rewards), reversed(dones)): R = r + self.gamma * R * (1 - done) returns.insert(0, R) return torch.tensor(returns, dtype=torch.float32).to(device) def collect_rollout(self): states, acts, rews, dones = [], [], [], [] old_logits = [] state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) ep_rew = 0 for _ in range(self.group_size): with torch.no_grad(): logits = self.actor(state) dist = Categorical(logits=logits) act = dist.sample() next_state, rew, terminated, truncated, _ = self.env.step(act.item()) done = terminated or truncated ep_rew += rew states.append(state) acts.append(act) rews.append(rew) dones.append(done) old_logits.append(logits) state = torch.FloatTensor(next_state).to(device) if not done else torch.FloatTensor(self.env.reset()[0]).to(device) if done: self.ep_rewards.append(ep_rew) ep_rew = 0 returns = self._calc_returns(rews, dones) advantages = (returns - returns.mean()) / (returns.std() + 1e-8) return ( torch.stack(states), torch.stack(acts), advantages, torch.stack(old_logits) ) def train(self, total_updates=300): self.actor.train() for update in trange(total_updates): states, actions, advantages, old_logits = self.collect_rollout() dataset = torch.utils.data.TensorDataset(states, actions, advantages, old_logits) loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True) policy_losses = [] kl_divergences = [] for _ in range(self.epochs): for batch in loader: s_batch, a_batch, adv_batch, old_logits_batch = batch new_logits = self.actor(s_batch) old_dist = Categorical(logits=old_logits_batch.detach()) new_dist = Categorical(logits=new_logits) logp_new = new_dist.log_prob(a_batch) logp_old = old_dist.log_prob(a_batch).detach() ratio = torch.exp(logp_new - logp_old) surr1 = ratio * adv_batch surr2 = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * adv_batch policy_loss = -torch.min(surr1, surr2).mean() kl = torch.distributions.kl.kl_divergence(old_dist, new_dist).mean() loss = policy_loss + self.beta * kl self.optimizer.zero_grad() loss.backward() torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5) self.optimizer.step() policy_losses.append(policy_loss.item()) kl_divergences.append(kl.item()) if self.ep_rewards: avg_rew = np.mean(self.ep_rewards[-20:]) wandb.log({ \"update\": update, \"avg_reward\": avg_rew, \"policy_loss\": np.mean(policy_losses), \"kl_divergence\": np.mean(kl_divergences) }) def test(env, actor, episodes=5, render=False): actor.eval() for ep in range(episodes): state, _ = env.reset() total_rew = 0 while True: if render: env.render() with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) logits = actor(state_tensor) act = torch.argmax(logits).item() state, rew, terminated, truncated, _ = env.step(act) total_rew += rew if terminated or truncated: print(f\"Test Episode {ep+1} | Reward: {total_rew}\") break def main(): wandb.init(project=\"grpo-cartpole\") env = gym.make('CartPole-v1') state_dim = env.observation_space.shape[0] action_dim = env.action_space.n actor = ActorNetwork(state_dim, action_dim).to(device) optimizer = optim.Adam(actor.parameters(), lr=3e-4) trainer = GRPOTrainer( env=env, actor=actor, optimizer=optimizer, clip_ratio=0.2, beta=0.001, gamma=0.99, epochs=10, batch_size=32, group_size=200 ) trainer.train(total_updates=1000) test_env = gym.make('CartPole-v1', render_mode='human') test(test_env, actor, episodes=3, render=True) env.close() if __name__ == \"__main__\": main()","title":"RL Ch5 GRPO"},{"location":"05/#rl-ch5-grpo","text":"","title":"RL Ch5 GRPO"},{"location":"05/#introduction-to-grpo","text":"GRPO (Group Relative Policy Optimization) is a novel reinforcement learning method proposed by DeepSeek, specifically designed for large language model (LLM) reinforcement learning. It builds upon the ideas of PPO (Proximal Policy Optimization) but eliminates the need for a critic model, simplifying the overall architecture.","title":"Introduction to GRPO"},{"location":"05/#core-concept-of-grpo","text":"The core idea of GRPO is to optimize the policy by comparing the outputs of the new policy with a group of outputs from the old policy. This \"group relative\" comparison ensures that the updates are more stable and reliable. Instead of relying on a value function to estimate the advantage, GRPO uses relative rewards computed from multiple samples.","title":"Core Concept of GRPO"},{"location":"05/#objective-function-of-grpo","text":"The objective function of GRPO can be formulated as: $$ L^{GRPO}(\\theta) = \\mathbb{E}\\left[ \\sum_{i = 1}^{G}\\left(\\min \\left(\\frac{\\pi_{\\theta}\\left(o_{i}\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(o_{i}\\right)} A_{i}, \\text{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{i}\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(o_{i}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right)\\right) - \\beta \\mathbb{D} {KL}\\left(\\pi {\\theta} | \\pi_{\\text{ref}}\\right) \\right] $$ Where: - $A_{i} = \\frac{r_{i} - \\mathrm{mean}({r_1, r_2, \\cdots, r_G})}{\\mathrm{std}({r_1, r_2, \\cdots, r_G})}$ is the normalized advantage computed from a group of samples. Please note that, in GRPO, we use this group-normalized reward instead of GAE. The latter requires a critic model, which we do not have in GRPO. - $\\pi_{\\theta}$ and $\\pi_{\\theta_{\\text{old}}}$ are the new and old policies, respectively. - $\\varepsilon$ is a hyperparameter that controls the range of policy updates. - $\\beta$ is a regularization parameter that controls the KL divergence between the new policy and a reference policy $\\pi_{\\text{ref}}$. This functions the same as the entropy regulation trick we talked about in the last chapter, it is just that GRPO uses KL divergence instead of simple entropy (KL divergence is just entropy difference).","title":"Objective Function of GRPO"},{"location":"05/#advantages-of-grpo","text":"No Critic Network Required : By eliminating the need for a critic network, GRPO simplifies the model architecture and reduces computational overhead. Stable Updates : The clipping mechanism ensures that policy updates are stable and prevent large, potentially destabilizing changes. Robustness to Noisy Rewards : GRPO is more robust to noisy reward signals, making it suitable for complex tasks where reward estimation can be challenging.","title":"Advantages of GRPO"},{"location":"05/#implementation","text":"GRPO does not behave as well as PPO, so we use a simpler scenario. At the same time, this also demonstrates how to use policy gradient on discreet action spaces. import gymnasium as gym import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.distributions import Categorical import wandb from tqdm import trange device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") class ActorNetwork(nn.Module): def __init__(self, state_dim, action_dim): super().__init__() self.net = nn.Sequential( nn.Linear(state_dim, 64), nn.Tanh(), nn.Linear(64, 64), nn.Tanh(), nn.Linear(64, action_dim) ) self.apply(self._init_weights) def _init_weights(self, m): if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, gain=1.0) nn.init.constant_(m.bias, 0.0) def forward(self, x): return self.net(x) class GRPOTrainer: def __init__(self, env, actor, optimizer, clip_ratio=0.2, beta=0.001, gamma=0.99, epochs=10, batch_size=32, group_size=200): self.env = env self.actor = actor self.optimizer = optimizer self.clip_ratio = clip_ratio self.beta = beta self.gamma = gamma self.epochs = epochs self.batch_size = batch_size self.group_size = group_size self.ep_rewards = [] def _calc_returns(self, rewards, dones): returns = [] R = 0 for r, done in zip(reversed(rewards), reversed(dones)): R = r + self.gamma * R * (1 - done) returns.insert(0, R) return torch.tensor(returns, dtype=torch.float32).to(device) def collect_rollout(self): states, acts, rews, dones = [], [], [], [] old_logits = [] state, _ = self.env.reset() state = torch.FloatTensor(state).to(device) ep_rew = 0 for _ in range(self.group_size): with torch.no_grad(): logits = self.actor(state) dist = Categorical(logits=logits) act = dist.sample() next_state, rew, terminated, truncated, _ = self.env.step(act.item()) done = terminated or truncated ep_rew += rew states.append(state) acts.append(act) rews.append(rew) dones.append(done) old_logits.append(logits) state = torch.FloatTensor(next_state).to(device) if not done else torch.FloatTensor(self.env.reset()[0]).to(device) if done: self.ep_rewards.append(ep_rew) ep_rew = 0 returns = self._calc_returns(rews, dones) advantages = (returns - returns.mean()) / (returns.std() + 1e-8) return ( torch.stack(states), torch.stack(acts), advantages, torch.stack(old_logits) ) def train(self, total_updates=300): self.actor.train() for update in trange(total_updates): states, actions, advantages, old_logits = self.collect_rollout() dataset = torch.utils.data.TensorDataset(states, actions, advantages, old_logits) loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True) policy_losses = [] kl_divergences = [] for _ in range(self.epochs): for batch in loader: s_batch, a_batch, adv_batch, old_logits_batch = batch new_logits = self.actor(s_batch) old_dist = Categorical(logits=old_logits_batch.detach()) new_dist = Categorical(logits=new_logits) logp_new = new_dist.log_prob(a_batch) logp_old = old_dist.log_prob(a_batch).detach() ratio = torch.exp(logp_new - logp_old) surr1 = ratio * adv_batch surr2 = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * adv_batch policy_loss = -torch.min(surr1, surr2).mean() kl = torch.distributions.kl.kl_divergence(old_dist, new_dist).mean() loss = policy_loss + self.beta * kl self.optimizer.zero_grad() loss.backward() torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5) self.optimizer.step() policy_losses.append(policy_loss.item()) kl_divergences.append(kl.item()) if self.ep_rewards: avg_rew = np.mean(self.ep_rewards[-20:]) wandb.log({ \"update\": update, \"avg_reward\": avg_rew, \"policy_loss\": np.mean(policy_losses), \"kl_divergence\": np.mean(kl_divergences) }) def test(env, actor, episodes=5, render=False): actor.eval() for ep in range(episodes): state, _ = env.reset() total_rew = 0 while True: if render: env.render() with torch.no_grad(): state_tensor = torch.FloatTensor(state).to(device) logits = actor(state_tensor) act = torch.argmax(logits).item() state, rew, terminated, truncated, _ = env.step(act) total_rew += rew if terminated or truncated: print(f\"Test Episode {ep+1} | Reward: {total_rew}\") break def main(): wandb.init(project=\"grpo-cartpole\") env = gym.make('CartPole-v1') state_dim = env.observation_space.shape[0] action_dim = env.action_space.n actor = ActorNetwork(state_dim, action_dim).to(device) optimizer = optim.Adam(actor.parameters(), lr=3e-4) trainer = GRPOTrainer( env=env, actor=actor, optimizer=optimizer, clip_ratio=0.2, beta=0.001, gamma=0.99, epochs=10, batch_size=32, group_size=200 ) trainer.train(total_updates=1000) test_env = gym.make('CartPole-v1', render_mode='human') test(test_env, actor, episodes=3, render=True) env.close() if __name__ == \"__main__\": main()","title":"Implementation"}]}